#!/bin/bash

# Configuration
SPARK_MASTER_CONTAINER="hydraulic-system-anomaly-detection-spark-master-1"
LABEL_PRODUCER_SCRIPT="src/producer_labels.py"
TRAINER_SCRIPT="/opt/spark-apps/spark_trainer.py"
MLFLOW_UI="http://localhost:5001"

echo "üöÄ Starting Measurement & Training Pipeline..."
echo "============================================="

# 1. Ingest Labels (Simulating API Trigger)
echo "üì¶ Step 1: Ingesting Labels..."
python $LABEL_PRODUCER_SCRIPT
if [ $? -ne 0 ]; then
    echo "‚ùå Label ingestion failed."
    exit 1
fi
echo "‚úÖ Labels ingested."

# 2. Wait for HDFS
echo "‚è≥ Step 2: Waiting 10s for HDFS ingestion..."
sleep 10

# 3. Trigger Training Job
echo "üß† Step 3: Triggering Spark Training Job..."
docker exec -u 0 $SPARK_MASTER_CONTAINER \
  /opt/spark/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mlflow:mlflow-spark:2.3.1 \
  $TRAINER_SCRIPT

if [ $? -eq 0 ]; then
    echo "============================================="
    echo "üéâ Pipeline Completed Successfully!"
    echo "üëâ View Model at: $MLFLOW_UI"
else
    echo "‚ùå Training Job Failed."
    exit 1
fi
