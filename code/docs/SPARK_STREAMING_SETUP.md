# üöÄ Spark Streaming Setup Guide
## Real-time Data Processing v·ªõi Spark Structured Streaming

H∆∞·ªõng d·∫´n chi ti·∫øt setup v√† s·ª≠ d·ª•ng Spark Streaming ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu real-time t·ª´ Kafka, ƒë·ªìng b·ªô h√≥a 17 sensors, v√† t√≠nh to√°n features.

---

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan](#1-t·ªïng-quan)
2. [Y√™u C·∫ßu H·ªá Th·ªëng](#2-y√™u-c·∫ßu-h·ªá-th·ªëng)
3. [C√†i ƒê·∫∑t Spark](#3-c√†i-ƒë·∫∑t-spark)
4. [C·∫•u H√¨nh Kafka Consumer](#4-c·∫•u-h√¨nh-kafka-consumer)
5. [Spark Streaming Application](#5-spark-streaming-application)
6. [ƒê·ªìng B·ªô H√≥a 17 Sensors](#6-ƒë·ªìng-b·ªô-h√≥a-17-sensors)
7. [T√≠nh To√°n Features](#7-t√≠nh-to√°n-features)
8. [Ghi D·ªØ Li·ªáu V√†o HDFS](#8-ghi-d·ªØ-li·ªáu-v√†o-hdfs)
9. [Monitoring & Debugging](#9-monitoring--debugging)
10. [Troubleshooting](#10-troubleshooting)

---

## 1. T·ªïng Quan

### 1.1. M·ª•c ƒê√≠ch

Spark Streaming ƒë∆∞·ª£c d√πng ƒë·ªÉ:
- ‚úÖ ƒê·ªçc d·ªØ li·ªáu real-time t·ª´ 17 Kafka topics
- ‚úÖ ƒê·ªìng b·ªô h√≥a messages t·ª´ c√°c sensors v·ªõi t·ªëc ƒë·ªô kh√°c nhau
- ‚úÖ T√≠nh to√°n features real-time (aggregation, statistics)
- ‚úÖ Ph√°t hi·ªán anomalies
- ‚úÖ Ghi d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o HDFS

### 1.2. Ki·∫øn Tr√∫c

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Kafka           ‚îÇ  17 topics (hydraulic-PS1, PS2, ..., SE)
‚îÇ  (17 topics)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Spark Structured Streaming
         ‚îÇ (subscribe all topics)
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Spark Streaming App        ‚îÇ
‚îÇ  - Window: 1 second         ‚îÇ
‚îÇ  - Join 17 sensors          ‚îÇ
‚îÇ  - Calculate features       ‚îÇ
‚îÇ  - Detect anomalies         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                 ‚îÇ                  ‚îÇ
         ‚ñº                 ‚ñº                  ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ HDFS     ‚îÇ    ‚îÇ Console  ‚îÇ      ‚îÇ Kafka    ‚îÇ
    ‚îÇ (Parquet)‚îÇ    ‚îÇ (logs)   ‚îÇ      ‚îÇ (alerts) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3. Lu·ªìng D·ªØ Li·ªáu

1. **Producer** g·ª≠i messages v√†o 17 Kafka topics
2. **Spark Streaming** subscribe t·∫•t c·∫£ topics
3. **Window-based processing** (1 gi√¢y) ƒë·ªÉ ƒë·ªìng b·ªô h√≥a
4. **Join & Aggregate** messages t·ª´ 17 sensors
5. **Calculate features** (mean, std, min, max, etc.)
6. **Write to HDFS** (Parquet format)

---

## 2. Y√™u C·∫ßu H·ªá Th·ªëng

### 2.1. Ph·∫ßn M·ªÅm

- **Java 8 ho·∫∑c 11** (required for Spark)
- **Python 3.8+** (cho PySpark)
- **Apache Spark 3.3.0+**
- **Kafka** (ƒë√£ c√≥ trong project)
- **HDFS** (s·∫Ω setup sau)

### 2.2. T√†i Nguy√™n

- **RAM:** T·ªëi thi·ªÉu 4GB (khuy·∫øn ngh·ªã 8GB+)
- **CPU:** 2+ cores
- **Disk:** 10GB+ free space

### 2.3. Dependencies

C·∫ßn th√™m v√†o `requirements.txt`:
```
pyspark>=3.3.0
kafka-python>=2.0.2
```

---

## 3. C√†i ƒê·∫∑t Spark

### 3.1. Download Spark

```bash
# T·∫°o th∆∞ m·ª•c cho Spark
mkdir -p ~/spark
cd ~/spark

# Download Spark 3.5.0 (ho·∫∑c version m·ªõi nh·∫•t)
wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz

# Gi·∫£i n√©n
tar -xzf spark-3.5.0-bin-hadoop3.tgz
cd spark-3.5.0-bin-hadoop3

# Set environment variables
export SPARK_HOME=$(pwd)
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

### 3.2. C·∫•u H√¨nh Spark

T·∫°o file `$SPARK_HOME/conf/spark-defaults.conf`:

```properties
# Spark configuration
spark.master                     local[2]
spark.app.name                   HydraulicSystemStreaming
spark.sql.streaming.checkpointLocation /tmp/spark-checkpoints
spark.sql.streaming.schemaInference true

# Kafka integration
spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0

# Memory settings
spark.driver.memory              2g
spark.executor.memory            2g
spark.driver.maxResultSize       1g

# Streaming settings
spark.sql.streaming.stopGracefullyOnShutdown true
spark.sql.streaming.streamingQueryListeners org.apache.spark.sql.streaming.StreamingQueryListener
```

### 3.3. Verify Installation

```bash
# Ki·ªÉm tra Spark
$SPARK_HOME/bin/spark-submit --version

# Ch·∫°y PySpark shell ƒë·ªÉ test
$SPARK_HOME/bin/pyspark
```

Trong PySpark shell:
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Test").getOrCreate()
print(spark.version)  # Should print 3.5.0
```

---

## 4. C·∫•u H√¨nh Kafka Consumer

### 4.1. Kafka Topics

ƒê·∫£m b·∫£o 17 topics ƒë√£ ƒë∆∞·ª£c t·∫°o:

```bash
# List topics
kafka-topics.sh --bootstrap-server localhost:29092 --list

# N·∫øu ch∆∞a c√≥, t·∫°o topics:
kafka-topics.sh --bootstrap-server localhost:29092 --create --topic hydraulic-PS1 --partitions 1 --replication-factor 1
# ... (t·∫°o cho t·∫•t c·∫£ 17 topics)
```

Ho·∫∑c d√πng script c√≥ s·∫µn:
```bash
cd scripts
python create_kafka_topics.py
```

### 4.2. Kafka Consumer Group

Spark s·∫Ω t·ª± ƒë·ªông t·∫°o consumer group. C√≥ th·ªÉ monitor:

```bash
# Xem consumer groups
kafka-consumer-groups.sh --bootstrap-server localhost:29092 --list

# Xem lag
kafka-consumer-groups.sh --bootstrap-server localhost:29092 \
  --group spark-streaming-group --describe
```

---

## 5. Spark Streaming Application

### 5.1. T·∫°o Spark Streaming App

T·∫°o file `src/spark_streaming_app.py`:

```python
#!/usr/bin/env python3
"""
Spark Structured Streaming Application
ƒê·ªçc d·ªØ li·ªáu t·ª´ 17 Kafka topics, ƒë·ªìng b·ªô h√≥a v√† t√≠nh to√°n features
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json
from datetime import datetime

# Kafka configuration
KAFKA_BROKER = "localhost:29092"
SENSOR_TOPICS = [
    "hydraulic-PS1", "hydraulic-PS2", "hydraulic-PS3",
    "hydraulic-PS4", "hydraulic-PS5", "hydraulic-PS6",
    "hydraulic-EPS1",
    "hydraulic-FS1", "hydraulic-FS2",
    "hydraulic-TS1", "hydraulic-TS2", "hydraulic-TS3", "hydraulic-TS4",
    "hydraulic-CE", "hydraulic-CP", "hydraulic-SE", "hydraulic-VS1"
]

# Schema cho Kafka messages
MESSAGE_SCHEMA = StructType([
    StructField("sensor", StringType(), True),
    StructField("cycle", IntegerType(), True),
    StructField("sample_idx", IntegerType(), True),
    StructField("value", DoubleType(), True),
    StructField("timestamp", StringType(), True),
    StructField("sampling_rate_hz", IntegerType(), True)
])


def create_spark_session():
    """T·∫°o SparkSession v·ªõi c·∫•u h√¨nh ph√π h·ª£p"""
    spark = SparkSession.builder \
        .appName("HydraulicSystemStreaming") \
        .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-checkpoints") \
        .config("spark.sql.streaming.schemaInference", "true") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def read_kafka_stream(spark, topic):
    """ƒê·ªçc stream t·ª´ 1 Kafka topic"""
    return spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BROKER) \
        .option("subscribe", topic) \
        .option("startingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()


def parse_kafka_message(df):
    """Parse JSON messages t·ª´ Kafka"""
    # Parse value column (JSON string)
    df_parsed = df.select(
        col("key").cast("string"),
        from_json(col("value").cast("string"), MESSAGE_SCHEMA).alias("data"),
        col("timestamp").alias("kafka_timestamp")
    )
    
    # Flatten nested structure
    df_flattened = df_parsed.select(
        col("data.sensor").alias("sensor"),
        col("data.cycle").alias("cycle"),
        col("data.sample_idx").alias("sample_idx"),
        col("data.value").alias("value"),
        to_timestamp(col("data.timestamp")).alias("event_timestamp"),
        col("data.sampling_rate_hz").alias("sampling_rate_hz"),
        col("kafka_timestamp")
    )
    
    return df_flattened


def aggregate_by_window(df, window_duration="1 second"):
    """Aggregate messages theo time window"""
    windowed = df \
        .withWatermark("event_timestamp", "10 seconds") \
        .groupBy(
            window("event_timestamp", window_duration),
            "cycle",
            "sensor"
        ) \
        .agg(
            collect_list("value").alias("values"),
            count("value").alias("count"),
            avg("value").alias("avg_value"),
            min("value").alias("min_value"),
            max("value").alias("max_value"),
            stddev("value").alias("std_value"),
            first("sample_idx").alias("first_sample_idx"),
            last("sample_idx").alias("last_sample_idx")
        )
    
    return windowed


def join_all_sensors(sensor_dfs):
    """Join t·∫•t c·∫£ sensors l·∫°i v·ªõi nhau theo time window"""
    # Start v·ªõi sensor ƒë·∫ßu ti√™n
    result = sensor_dfs[0]
    
    # Join v·ªõi c√°c sensors c√≤n l·∫°i
    for i in range(1, len(sensor_dfs)):
        result = result.join(
            sensor_dfs[i],
            ["window", "cycle"],
            "outer"
        )
    
    return result


def calculate_features(df):
    """T√≠nh to√°n features t·ª´ d·ªØ li·ªáu ƒë√£ join"""
    # Th√™m c√°c features m·ªõi
    df_features = df \
        .withColumn("window_start", col("window.start")) \
        .withColumn("window_end", col("window.end")) \
        .withColumn("pressure_avg", 
            (col("PS1_avg_value") + col("PS2_avg_value") + 
             col("PS3_avg_value") + col("PS4_avg_value") + 
             col("PS5_avg_value") + col("PS6_avg_value")) / 6
        ) \
        .withColumn("pressure_range",
            col("PS1_max_value") - col("PS1_min_value")
        ) \
        .withColumn("temperature_avg",
            (col("TS1_avg_value") + col("TS2_avg_value") + 
             col("TS3_avg_value") + col("TS4_avg_value")) / 4
        )
    
    return df_features


def detect_anomalies(df):
    """Ph√°t hi·ªán anomalies d·ª±a tr√™n thresholds"""
    # Define thresholds
    PRESSURE_MAX = 300.0  # bar
    TEMPERATURE_MAX = 100.0  # ¬∞C
    
    anomalies = df \
        .filter(
            (col("PS1_avg_value") > PRESSURE_MAX) |
            (col("temperature_avg") > TEMPERATURE_MAX) |
            (col("pressure_range") > 50.0)
        ) \
        .withColumn("anomaly_type", 
            when(col("PS1_avg_value") > PRESSURE_MAX, "high_pressure")
            .when(col("temperature_avg") > TEMPERATURE_MAX, "high_temperature")
            .otherwise("pressure_range_anomaly")
        )
    
    return anomalies


def write_to_hdfs(df, output_path, format="parquet"):
    """Ghi d·ªØ li·ªáu v√†o HDFS"""
    query = df \
        .writeStream \
        .format(format) \
        .option("path", output_path) \
        .option("checkpointLocation", f"/tmp/spark-checkpoints/hdfs") \
        .partitionBy("cycle", "year", "month", "day", "hour", "minute") \
        .outputMode("append") \
        .trigger(processingTime="1 second") \
        .start()
    
    return query


def write_to_console(df):
    """Ghi d·ªØ li·ªáu ra console ƒë·ªÉ debug"""
    query = df \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", "false") \
        .trigger(processingTime="5 seconds") \
        .start()
    
    return query


def main():
    """Main function"""
    print("=" * 80)
    print("üöÄ Spark Streaming Application - Hydraulic System")
    print("=" * 80)
    
    # T·∫°o SparkSession
    spark = create_spark_session()
    
    print(f"‚úÖ Spark version: {spark.version}")
    print(f"‚úÖ Kafka broker: {KAFKA_BROKER}")
    print(f"‚úÖ Topics: {len(SENSOR_TOPICS)}")
    
    # ƒê·ªçc streams t·ª´ t·∫•t c·∫£ topics
    print("\nüìä Reading streams from Kafka topics...")
    sensor_streams = {}
    
    for topic in SENSOR_TOPICS:
        print(f"  - Reading {topic}...")
        df_raw = read_kafka_stream(spark, topic)
        df_parsed = parse_kafka_message(df_raw)
        df_aggregated = aggregate_by_window(df_parsed)
        
        # Rename columns v·ªõi prefix sensor name
        sensor_name = topic.replace("hydraulic-", "")
        df_renamed = df_aggregated.select(
            col("window"),
            col("cycle"),
            col(f"{sensor_name}_values").alias(f"{sensor_name}_values"),
            col("count").alias(f"{sensor_name}_count"),
            col("avg_value").alias(f"{sensor_name}_avg_value"),
            col("min_value").alias(f"{sensor_name}_min_value"),
            col("max_value").alias(f"{sensor_name}_max_value"),
            col("std_value").alias(f"{sensor_name}_std_value")
        )
        
        sensor_streams[sensor_name] = df_renamed
    
    # Join t·∫•t c·∫£ sensors
    print("\nüîó Joining all sensors...")
    # Note: Spark Streaming kh√¥ng support join tr·ª±c ti·∫øp nhi·ªÅu streams
    # C·∫ßn d√πng approach kh√°c (xem ph·∫ßn sau)
    
    # T·∫°m th·ªùi x·ª≠ l√Ω t·ª´ng sensor ri√™ng
    print("\nüíæ Writing to HDFS...")
    queries = []
    
    for sensor_name, df in sensor_streams.items():
        output_path = f"hdfs://localhost:9000/hydraulic_data/streaming/{sensor_name}"
        query = write_to_hdfs(df, output_path)
        queries.append(query)
        print(f"  ‚úÖ Started query for {sensor_name}")
    
    # Ho·∫∑c ghi ra console ƒë·ªÉ test
    print("\nüì∫ Writing to console (for testing)...")
    test_query = write_to_console(sensor_streams["PS1"])
    queries.append(test_query)
    
    # Ch·ªù queries
    print("\n‚è≥ Waiting for streaming queries...")
    print("Press Ctrl+C to stop\n")
    
    try:
        for query in queries:
            query.awaitTermination()
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Stopping streaming queries...")
        for query in queries:
            query.stop()
        
        print("‚úÖ All queries stopped")
        spark.stop()


if __name__ == "__main__":
    main()
```

### 5.2. Ch·∫°y Spark Streaming App

```bash
# Ch·∫°y v·ªõi spark-submit
$SPARK_HOME/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  --master local[2] \
  src/spark_streaming_app.py
```

---

## 6. ƒê·ªìng B·ªô H√≥a 17 Sensors

### 6.1. V·∫•n ƒê·ªÅ

Messages t·ª´ 17 sensors ƒë·∫øn kh√¥ng ƒë·ªìng b·ªô:
- PS1 (100Hz): 100 messages/gi√¢y
- FS1 (10Hz): 10 messages/gi√¢y
- TS1 (1Hz): 1 message/gi√¢y

### 6.2. Gi·∫£i Ph√°p: Window-based Aggregation

```python
def synchronize_sensors(spark):
    """ƒê·ªìng b·ªô h√≥a t·∫•t c·∫£ sensors b·∫±ng window"""
    
    # ƒê·ªçc t·∫•t c·∫£ topics c√πng l√∫c
    df_all = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BROKER) \
        .option("subscribe", ",".join(SENSOR_TOPICS)) \
        .option("startingOffsets", "latest") \
        .load()
    
    # Parse messages
    df_parsed = parse_kafka_message(df_all)
    
    # Pivot ƒë·ªÉ c√≥ m·ªói sensor l√† 1 column
    df_pivoted = df_parsed \
        .withWatermark("event_timestamp", "10 seconds") \
        .groupBy(
            window("event_timestamp", "1 second"),
            "cycle"
        ) \
        .pivot("sensor") \
        .agg(
            collect_list("value").alias("values"),
            avg("value").alias("avg"),
            min("value").alias("min"),
            max("value").alias("max")
        )
    
    return df_pivoted
```

### 6.3. Interpolation cho Sensors Ch·∫≠m

```python
def interpolate_slow_sensors(df):
    """Interpolate sensors ch·∫≠m (1Hz, 10Hz) ƒë·ªÉ match v·ªõi 100Hz"""
    from pyspark.sql.window import Window
    
    # Forward fill cho sensors ch·∫≠m
    window_spec = Window \
        .partitionBy("cycle") \
        .orderBy("window") \
        .rowsBetween(Window.unboundedPreceding, Window.currentRow)
    
    df_interpolated = df \
        .withColumn("TS1_interpolated", 
            last("TS1_avg", ignorenulls=True).over(window_spec)
        ) \
        .withColumn("FS1_interpolated",
            last("FS1_avg", ignorenulls=True).over(window_spec)
        )
    
    return df_interpolated
```

---

## 7. T√≠nh To√°n Features

### 7.1. Statistical Features

```python
def calculate_statistical_features(df):
    """T√≠nh to√°n statistical features"""
    df_features = df \
        .withColumn("pressure_mean", 
            (col("PS1_avg") + col("PS2_avg") + col("PS3_avg") + 
             col("PS4_avg") + col("PS5_avg") + col("PS6_avg")) / 6
        ) \
        .withColumn("pressure_std",
            sqrt(
                (col("PS1_std")**2 + col("PS2_std")**2 + 
                 col("PS3_std")**2 + col("PS4_std")**2 + 
                 col("PS5_std")**2 + col("PS6_std")**2) / 6
            )
        ) \
        .withColumn("temperature_mean",
            (col("TS1_avg") + col("TS2_avg") + 
             col("TS3_avg") + col("TS4_avg")) / 4
        ) \
        .withColumn("flow_mean",
            (col("FS1_avg") + col("FS2_avg")) / 2
        )
    
    return df_features
```

### 7.2. Time-domain Features

```python
def calculate_time_domain_features(df):
    """T√≠nh to√°n time-domain features"""
    from pyspark.sql.window import Window
    
    window_spec = Window \
        .partitionBy("cycle") \
        .orderBy("window") \
        .rowsBetween(Window.unboundedPreceding, Window.currentRow)
    
    df_time_features = df \
        .withColumn("pressure_rate_of_change",
            (col("pressure_mean") - 
             lag("pressure_mean", 1).over(window_spec)) / 1.0
        ) \
        .withColumn("pressure_rolling_mean",
            avg("pressure_mean").over(
                window_spec.rowsBetween(-5, Window.currentRow)
            )
        )
    
    return df_time_features
```

### 7.3. Cross-sensor Features

```python
def calculate_cross_sensor_features(df):
    """T√≠nh to√°n features gi·ªØa c√°c sensors"""
    df_cross = df \
        .withColumn("pressure_temperature_ratio",
            col("pressure_mean") / (col("temperature_mean") + 1e-6)
        ) \
        .withColumn("flow_pressure_product",
            col("flow_mean") * col("pressure_mean")
        ) \
        .withColumn("efficiency_index",
            col("SE_avg") * col("CE_avg") / 100.0
        )
    
    return df_cross
```

---

## 8. Ghi D·ªØ Li·ªáu V√†o HDFS

### 8.1. C·∫•u Tr√∫c Th∆∞ M·ª•c

```
hdfs://localhost:9000/hydraulic_data/
  /streaming/
    /year=2025/
      /month=11/
        /day=08/
          /hour=14/
            /minute=30/
              /second=45/
                part-00000.parquet
```

### 8.2. Code Ghi V√†o HDFS

```python
def write_to_hdfs_with_partitioning(df, base_path):
    """Ghi v√†o HDFS v·ªõi partitioning theo th·ªùi gian"""
    
    # Th√™m partition columns
    df_partitioned = df \
        .withColumn("year", year("window_start")) \
        .withColumn("month", month("window_start")) \
        .withColumn("day", dayofmonth("window_start")) \
        .withColumn("hour", hour("window_start")) \
        .withColumn("minute", minute("window_start")) \
        .withColumn("second", second("window_start"))
    
    # Ghi v√†o HDFS
    query = df_partitioned \
        .writeStream \
        .format("parquet") \
        .option("path", base_path) \
        .option("checkpointLocation", "/tmp/spark-checkpoints/hdfs") \
        .partitionBy("year", "month", "day", "hour", "minute", "second", "cycle") \
        .outputMode("append") \
        .trigger(processingTime="1 second") \
        .start()
    
    return query
```

### 8.3. Verify Data in HDFS

```bash
# List files
hdfs dfs -ls -R /hydraulic_data/streaming/

# Read sample data
hdfs dfs -cat /hydraulic_data/streaming/year=2025/month=11/day=08/hour=14/minute=30/second=45/part-00000.parquet | head
```

---

## 9. Monitoring & Debugging

### 9.1. Spark UI

Truy c·∫≠p Spark UI:
```
http://localhost:4040
```

Xem:
- Streaming queries
- Jobs & stages
- Metrics
- Query progress

### 9.2. Logging

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Trong code
logger.info(f"Processing {df.count()} records")
```

### 9.3. Metrics

```python
def monitor_query(query):
    """Monitor streaming query"""
    while query.isActive:
        print(f"Status: {query.status}")
        print(f"Recent progress: {query.recentProgress}")
        time.sleep(5)
```

---

## 10. Troubleshooting

### 10.1. Kafka Connection Issues

**L·ªói:** `Failed to connect to Kafka`

**Gi·∫£i ph√°p:**
```bash
# Ki·ªÉm tra Kafka ƒëang ch·∫°y
docker ps | grep kafka

# Test connection
kafka-console-consumer.sh --bootstrap-server localhost:29092 --topic hydraulic-PS1 --from-beginning
```

### 10.2. Memory Issues

**L·ªói:** `OutOfMemoryError`

**Gi·∫£i ph√°p:**
- TƒÉng memory trong `spark-defaults.conf`:
```properties
spark.driver.memory 4g
spark.executor.memory 4g
```

### 10.3. Checkpoint Issues

**L·ªói:** `Checkpoint directory not found`

**Gi·∫£i ph√°p:**
```bash
# T·∫°o checkpoint directory
mkdir -p /tmp/spark-checkpoints
chmod 777 /tmp/spark-checkpoints
```

### 10.4. Schema Mismatch

**L·ªói:** `Schema mismatch`

**Gi·∫£i ph√°p:**
- Ki·ªÉm tra message format t·ª´ producer
- Verify MESSAGE_SCHEMA matches actual data

---

## 11. Best Practices

1. **Watermark:** Lu√¥n d√πng watermark ƒë·ªÉ x·ª≠ l√Ω late data
2. **Checkpointing:** Enable checkpoint ƒë·ªÉ recovery
3. **Partitioning:** Partition theo th·ªùi gian ƒë·ªÉ query nhanh
4. **Monitoring:** Monitor Spark UI v√† logs
5. **Testing:** Test v·ªõi console output tr∆∞·ªõc khi ghi HDFS

---

## 12. Next Steps

Sau khi setup Spark Streaming:
1. ‚úÖ Verify data flow
2. ‚úÖ Test v·ªõi 1-2 sensors tr∆∞·ªõc
3. ‚úÖ Scale l√™n 17 sensors
4. ‚úÖ T·ªëi ∆∞u performance
5. ‚úÖ Setup monitoring alerts

---

## üìö References

- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [HDFS Integration](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)

