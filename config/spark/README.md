# âš¡ Spark Configuration

ThÆ° má»¥c nÃ y chá»©a cÃ¡c cáº¥u hÃ¬nh cho Apache Spark Structured Streaming.

## ğŸ“ Files

- **`spark-defaults.conf`** - Spark configuration

  - Memory settings
  - Streaming settings
  - Kafka integration
  - Adaptive query execution

- **`log4j.properties`** - Logging configuration
  - Log levels
  - Console output format
  - Suppress noisy loggers

## ğŸš€ Usage

### Quick Start

Xem hÆ°á»›ng dáº«n chi tiáº¿t: **[docs/SPARK_STREAMING.md](../docs/SPARK_STREAMING.md)**

### Local Development

```bash
# Install PySpark
pip install pyspark

# Run locally
./scripts/run_spark_streaming_local.sh
```

### Cluster Mode

```bash
# Start Spark cluster
docker-compose -f docker-compose.khang.yml up -d spark-master spark-worker

# Submit job
./scripts/submit_spark_streaming.sh
```

## ğŸ“Š Features

Spark Structured Streaming consumer:

- âœ… **Real-time aggregations** - 1-minute windows
- âœ… **Per-sensor metrics** - Count, avg, max, min, sum
- âœ… **Watermark handling** - Late data support
- âœ… **Fault tolerance** - Checkpoint-based recovery
- âœ… **Scalable** - Distributed processing

## ğŸ”§ Configuration

### Memory Settings

Default in `spark-defaults.conf`:

- Driver: 2GB
- Executor: 2GB

Adjust based on your data volume.

### Window Size

Currently: **1 minute windows**

Modify in `src/spark_streaming_consumer.py`:

```python
.groupBy(window("event_timestamp", "1 minute"), "sensor")
```

### Checkpoint Location

Default: `/tmp/spark-checkpoints/hydraulic-streaming`

Change in `spark-defaults.conf`:

```conf
spark.sql.streaming.checkpointLocation    /your/path
```

## ğŸ“ˆ Output

Spark Streaming outputs:

- **Console** - For monitoring (every 10 seconds)
- **Memory table** - Queryable via Spark SQL (`hydraulic_aggregations`)

## ğŸ”— Related Files

- **`src/spark_streaming_consumer.py`** - Main application
- **`scripts/submit_spark_streaming.sh`** - Submit to cluster
- **`scripts/run_spark_streaming_local.sh`** - Run locally
- **`docs/SPARK_STREAMING.md`** - Complete guide

## ğŸ³ Docker Setup

Docker Compose services:

- `spark-master` (port 7077, 8080)
- `spark-worker`

Start with:

```bash
docker-compose -f docker-compose.khang.yml up -d spark-master spark-worker
```

Monitor at: http://localhost:8080

---

**Status:** âœ… Ready to use! See [SPARK_STREAMING.md](../docs/SPARK_STREAMING.md) for details.
