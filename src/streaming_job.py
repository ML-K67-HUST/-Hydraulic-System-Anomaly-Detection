#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Spark Structured Streaming Job for Hydraulic System
Äá»c tá»« Kafka, xá»­ lÃ½ vá»›i windowing/watermarking, phÃ¡t hiá»‡n báº¥t thÆ°á»ng,
vÃ  ghi Ä‘á»“ng thá»i ra HDFS (cold) vÃ  MongoDB (hot).
"""

import yaml
import json
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window, avg, max, min, when, current_date, to_timestamp
from pyspark.sql.types import StructType


def load_config(config_path='config/spark/streaming.yaml'):
    """Táº£i tá»‡p cáº¥u hÃ¬nh YAML tá»« Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i so vá»›i gá»‘c project."""
    if not os.path.exists(config_path):
        alt_path = f"../{config_path}"
        if os.path.exists(alt_path):
            config_path = alt_path
        else:
            raise FileNotFoundError(f"KhÃ´ng thá»ƒ tÃ¬m tháº¥y tá»‡p cáº¥u hÃ¬nh: {config_path}")
            
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def create_spark_session(app_name, master, spark_ui_port):
    """Khá»Ÿi táº¡o vÃ  tráº£ vá» má»™t SparkSession."""
    spark = SparkSession \
        .builder \
        .appName(app_name) \
        .master(master) \
        .config("spark.ui.port", spark_ui_port) \
        .config("spark.sql.shuffle.partitions", 200) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    return spark


def load_kafka_schema(schema_path):
    """Táº£i schema JSON tá»« má»™t tá»‡p."""
    # Handle relative paths
    if not os.path.exists(schema_path):
        alt_path = f"../{schema_path}"
        if os.path.exists(alt_path):
            schema_path = alt_path
        else:
            raise FileNotFoundError(f"KhÃ´ng thá»ƒ tÃ¬m tháº¥y schema file: {schema_path}")
    
    try:
        with open(schema_path, 'r', encoding='utf-8') as f:
            schema_definition = json.load(f)
        print(f"ÄÃ£ táº£i schema thÃ nh cÃ´ng tá»«: {schema_path}")
        return StructType.fromJson(schema_definition)
    except Exception as e:
        print(f"Lá»—i khi táº£i schema tá»« {schema_path}: {e}")
        raise


def read_kafka_stream(spark, config):
    """Äá»c vÃ  phÃ¢n tÃ­ch cÃº phÃ¡p luá»“ng dá»¯ liá»‡u tá»« Kafka."""
    json_schema = load_kafka_schema(config['kafka']['schema_path'])
    
    # Use bootstrap_servers_host if available (for local development)
    kafka_bootstrap = config['kafka'].get('bootstrap_servers_host') or config['kafka']['bootstrap_servers']
    
    kafka_stream_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_bootstrap) \
        .option("subscribe", config['kafka']['subscribe_topic']) \
        .option("startingOffsets", config['kafka']['starting_offsets']) \
        .load()

    parsed_df = kafka_stream_df \
        .select(col("value").cast("string").alias("json_value")) \
        .withColumn("data", from_json(col("json_value"), json_schema)) \
        .select("data.*")  # Má»Ÿ rá»™ng cÃ¡c trÆ°á»ng JSON
    
    # Chuyá»ƒn Ä‘á»•i timestamp - tÃ¬m cá»™t timestamp trong schema
    timestamp_col = None
    for col_name in parsed_df.columns:
        if 'time' in col_name.lower() or 'timestamp' in col_name.lower():
            timestamp_col = col_name
            break
    
    if timestamp_col:
        parsed_df = parsed_df.withColumn("timestamp", to_timestamp(col(timestamp_col)))
    else:
        # Náº¿u khÃ´ng cÃ³ timestamp, táº¡o tá»« current time
        from pyspark.sql.functions import current_timestamp
        parsed_df = parsed_df.withColumn("timestamp", current_timestamp())
    
    print("ÄÃ£ thiáº¿t láº­p luá»“ng Ä‘á»c Kafka vÃ  phÃ¢n tÃ­ch cÃº phÃ¡p JSON.")
    return parsed_df


def apply_stream_processing(raw_df, logic_config):
    """Ãp dá»¥ng windowing, watermarking vÃ  tá»•ng há»£p."""
    print(f"Ãp dá»¥ng Window: {logic_config['window_duration']}, Slide: {logic_config['slide_duration']}, Watermark: {logic_config['watermark_delay']}")
    
    # XÃ¡c Ä‘á»‹nh device_id column
    device_col = None
    for col_name in raw_df.columns:
        if 'device' in col_name.lower() or 'sensor' in col_name.lower():
            device_col = col_name
            break
    
    if not device_col:
        # Táº¡o device_id tá»« sensor name náº¿u cÃ³
        if 'sensor' in raw_df.columns:
            device_col = 'sensor'
        else:
            # Fallback: táº¡o device_id = "hydraulic_system"
            from pyspark.sql.functions import lit
            raw_df = raw_df.withColumn("device_id", lit("hydraulic_system"))
            device_col = "device_id"
    
    # XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t Ä‘á»ƒ aggregate
    agg_exprs = []
    
    # Pressure
    pressure_cols = [c for c in raw_df.columns if 'pressure' in c.lower() or 'ps' in c.lower() or c == 'value']
    if pressure_cols:
        agg_exprs.append(avg(col(pressure_cols[0])).alias("avg_pressure"))
        agg_exprs.append(max(col(pressure_cols[0])).alias("max_pressure"))
    
    # Temperature
    temp_cols = [c for c in raw_df.columns if 'temperature' in c.lower() or 'temp' in c.lower()]
    if temp_cols:
        agg_exprs.append(max(col(temp_cols[0])).alias("max_temperature"))
    
    # Cooler efficiency
    cooler_cols = [c for c in raw_df.columns if 'cooler' in c.lower() or 'efficiency' in c.lower()]
    if cooler_cols:
        agg_exprs.append(min(col(cooler_cols[0])).alias("min_cooler_efficiency"))
    
    # ThÃªm timestamp
    agg_exprs.append(max(col("timestamp")).alias("latest_timestamp_in_window"))
    
    if not agg_exprs:
        # Fallback: aggregate táº¥t cáº£ numeric columns
        numeric_cols = [c for c, t in zip(raw_df.columns, raw_df.dtypes) if t[1] in ['int', 'bigint', 'float', 'double']]
        for col_name in numeric_cols[:5]:  # Limit to 5 columns
            agg_exprs.append(avg(col(col_name)).alias(f"avg_{col_name}"))
    
    # Logic tá»« Phase 2: Implement streaming job vá»›i windowing 
    windowed_aggregates_df = raw_df \
        .withWatermark("timestamp", logic_config['watermark_delay']) \
        .groupBy(
            col(device_col).alias("device_id"),  # NhÃ³m theo thiáº¿t bá»‹
            window(col("timestamp"), 
                   logic_config['window_duration'], 
                   logic_config['slide_duration']).alias("window")
        ) \
        .agg(*agg_exprs)
    
    return windowed_aggregates_df


def apply_anomaly_rules(processed_df, rules_config):
    """Ãp dá»¥ng cÃ¡c quy táº¯c phÃ¡t hiá»‡n báº¥t thÆ°á»ng Giai Ä‘oáº¡n 1."""
    print(f"Ãp dá»¥ng quy táº¯c: MaxPressure={rules_config['max_pressure']}, MaxTemp={rules_config['max_temperature']}")
    
    # Logic tá»« TrÃ¡ch nhiá»‡m: Implement anomaly detection rules 
    rules_applied_df = processed_df \
        .withColumn("is_pressure_anomaly", 
            when(col("avg_pressure").isNotNull(), col("avg_pressure") > rules_config['max_pressure'])
            .otherwise(False)
        ) \
        .withColumn("is_temp_anomaly", 
            when(col("max_temperature").isNotNull(), col("max_temperature") > rules_config['max_temperature'])
            .otherwise(False)
        ) \
        .withColumn("is_cooler_anomaly",
            when(col("min_cooler_efficiency").isNotNull(), 
                 col("min_cooler_efficiency") < rules_config.get('min_cooler_efficiency', 0.5))
            .otherwise(False)
        ) \
        .withColumn("rule_based_anomaly", 
            col("is_pressure_anomaly") | col("is_temp_anomaly") | col("is_cooler_anomaly")
        ) \
        .withColumn("alert_type", 
            when(col("is_pressure_anomaly"), "High Pressure")
            .when(col("is_temp_anomaly"), "High Temperature")
            .when(col("is_cooler_anomaly"), "Low Cooler Efficiency")
            .otherwise(None)
        )
    
    return rules_applied_df


def get_sink_writer(config):
    """HÃ m ná»™i bá»™ Ä‘á»ƒ xá»­ lÃ½ logic ghi `foreachBatch`."""
    
    class SinkWriter:
        def __init__(self, config):
            self.config = config
            print("SinkWriter Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o.")

        def write_batch(self, micro_batch_df, batch_id):
            """
            HÃ m Ä‘Æ°á»£c gá»i bá»Ÿi foreachBatch.
            Ghi Ä‘á»“ng thá»i ra HDFS (Cold) vÃ  MongoDB (Hot). 
            """
            print(f"--- Báº¯t Ä‘áº§u xá»­ lÃ½ Micro-Batch ID: {batch_id} ---")
            micro_batch_df.persist()  # Cache Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng

            # --- 1. Ghi vÃ o Luá»“ng Láº¡nh (HDFS/Parquet) ---
            # DÃ nh cho NgÆ°á»i 3 (Batch) vÃ  NgÆ°á»i 4 (ML) 
            if self.config['sinks']['cold_sink_hdfs'].get('enabled', True):
                try:
                    hdfs_config = self.config['sinks']['cold_sink_hdfs']
                    partition_col = hdfs_config.get('partition_by')
                    
                    df_to_write = micro_batch_df
                    if partition_col:
                        df_to_write = micro_batch_df.withColumn(partition_col, current_date())

                    writer = df_to_write.write.mode("append").format(hdfs_config.get('format', 'parquet'))
                    if partition_col:
                        writer = writer.partitionBy(partition_col)
                    
                    writer.save(hdfs_config['path'])
                    print(f"Batch {batch_id}: Ghi thÃ nh cÃ´ng vÃ o HDFS: {hdfs_config['path']}")
                    
                except Exception as e:
                    print(f"Batch {batch_id}: Lá»–I khi ghi vÃ o HDFS: {e}")
                    import traceback
                    traceback.print_exc()

            # --- 2. Ghi vÃ o Luá»“ng NÃ³ng (MongoDB) ---
            # DÃ nh cho truy váº¥n nhanh vÃ  cáº£nh bÃ¡o 
            if self.config['sinks']['hot_sink_mongo'].get('enabled', True):
                mongo_config = self.config['sinks']['hot_sink_mongo']
                # Use uri_host if available (for local development)
                mongo_uri = mongo_config.get('uri_host') or mongo_config['uri']
                mongo_db = mongo_config['database']

                # a. Ghi Alerts (Chá»‰ ghi náº¿u cÃ³ báº¥t thÆ°á»ng)
                try:
                    alerts_df = micro_batch_df.filter(col("rule_based_anomaly") == True) \
                        .select("window", "device_id", "alert_type", "avg_pressure", "max_temperature", "min_cooler_efficiency", "latest_timestamp_in_window")
                    
                    if not alerts_df.rdd.isEmpty():
                        alerts_df.write \
                            .format("mongo") \
                            .mode("append") \
                            .option("uri", mongo_uri) \
                            .option("database", mongo_db) \
                            .option("collection", mongo_config['collection_alerts']) \
                            .save()
                        print(f"Batch {batch_id}: Ghi thÃ nh cÃ´ng {alerts_df.count()} ALERTS vÃ o MongoDB.")
                    else:
                        print(f"Batch {batch_id}: KhÃ´ng cÃ³ alerts má»›i Ä‘á»ƒ ghi.")
                        
                except Exception as e:
                    print(f"Batch {batch_id}: Lá»–I khi ghi ALERTS vÃ o MongoDB: {e}")
                    import traceback
                    traceback.print_exc()

                # b. Ghi Metrics má»›i nháº¥t (Ghi táº¥t cáº£)
                try:
                    metrics_df = micro_batch_df.select("window", "device_id", "avg_pressure", "max_temperature", "min_cooler_efficiency", "latest_timestamp_in_window")
                    
                    metrics_df.write \
                        .format("mongo") \
                        .mode("append") \
                        .option("uri", mongo_uri) \
                        .option("database", mongo_db) \
                        .option("collection", mongo_config['collection_metrics']) \
                        .save()
                    print(f"Batch {batch_id}: Ghi thÃ nh cÃ´ng {metrics_df.count()} METRICS vÃ o MongoDB.")
                    
                except Exception as e:
                    print(f"Batch {batch_id}: Lá»–I khi ghi METRICS vÃ o MongoDB: {e}")
                    import traceback
                    traceback.print_exc()

            micro_batch_df.unpersist()
            print(f"--- Káº¿t thÃºc Micro-Batch ID: {batch_id} ---")

    # Tráº£ vá» hÃ m (function) mÃ  foreachBatch cáº§n
    return SinkWriter(config).write_batch


def main():
    # Táº£i cáº¥u hÃ¬nh
    config = load_config()
    spark_config = config['spark_app']
    
    # Khá»Ÿi táº¡o Spark
    spark = create_spark_session(
        spark_config['name'],
        spark_config['master'],
        spark_config.get('spark_ui_port', '4040')
    )
    spark.sparkContext.setLogLevel("WARN")  # Äáº·t log level theo file log4j.properties
    print("Spark Session Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o.")
    
    # 1. Äá»c luá»“ng Kafka 
    raw_stream_df = read_kafka_stream(spark, config)
    
    # 2. Xá»­ lÃ½ Tráº¡ng thÃ¡i (Windowing & Watermarking) 
    processed_stream_df = apply_stream_processing(raw_stream_df, config['logic'])
    
    # 3. Ãp dá»¥ng Logic PhÃ¡t hiá»‡n Báº¥t thÆ°á»ng (Giai Ä‘oáº¡n 1) 
    final_stream_df = apply_anomaly_rules(processed_stream_df, config['rules'])
    
    # 4. Thiáº¿t láº­p Ghi Dá»¯ liá»‡u (Dual Sink) báº±ng foreachBatch 
    sink_writer_logic = get_sink_writer(config)
    
    # 5. Khá»Ÿi cháº¡y Truy váº¥n
    checkpoint_location = config['processing']['checkpoint_location']
    # Náº¿u checkpoint location lÃ  HDFS nhÆ°ng khÃ´ng cÃ³ HDFS, dÃ¹ng local
    if checkpoint_location.startswith("hdfs://"):
        checkpoint_location = "/tmp/spark-checkpoints/streaming_job"
        print(f"Warning: HDFS not available, using local checkpoint: {checkpoint_location}")
    
    streaming_query = final_stream_df.writeStream \
        .foreachBatch(sink_writer_logic) \
        .outputMode("update") \
        .trigger(processingTime=config['processing']['trigger_interval']) \
        .option("checkpointLocation", checkpoint_location) \
        .start()
       
    print(f"Truy váº¥n Streaming Ä‘Ã£ báº¯t Ä‘áº§u. Checkpoint táº¡i: {checkpoint_location}")
    print(f"Spark UI cÃ³ thá»ƒ truy cáº­p táº¡i: http://localhost:{spark_config.get('spark_ui_port', '4040')} (hoáº·c cá»•ng UI cá»§a Spark Master)")
    print("Press Ctrl+C to stop")
    
    try:
        streaming_query.awaitTermination()
    except KeyboardInterrupt:
        print("\nðŸ›‘ Stopping streaming query...")
        streaming_query.stop()
        spark.stop()
        print("âœ… Stopped successfully")


if __name__ == "__main__":
    main()
