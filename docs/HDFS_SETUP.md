# üíæ HDFS Setup Guide
## Distributed Storage cho Batch Processing v√† Training Dataset

H∆∞·ªõng d·∫´n chi ti·∫øt setup HDFS ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu batch, t·∫°o training dataset t·ª´ d·ªØ li·ªáu ƒë√£ gom, v√† join v·ªõi labels.

---

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan](#1-t·ªïng-quan)
2. [Y√™u C·∫ßu H·ªá Th·ªëng](#2-y√™u-c·∫ßu-h·ªá-th·ªëng)
3. [C√†i ƒê·∫∑t HDFS](#3-c√†i-ƒë·∫∑t-hdfs)
4. [C·∫•u H√¨nh HDFS](#4-c·∫•u-h√¨nh-hdfs)
5. [C·∫•u Tr√∫c D·ªØ Li·ªáu](#5-c·∫•u-tr√∫c-d·ªØ-li·ªáu)
6. [Ghi D·ªØ Li·ªáu V√†o HDFS](#6-ghi-d·ªØ-li·ªáu-v√†o-hdfs)
7. [ƒê·ªçc D·ªØ Li·ªáu T·ª´ HDFS](#7-ƒë·ªçc-d·ªØ-li·ªáu-t·ª´-hdfs)
8. [T·∫°o Training Dataset](#8-t·∫°o-training-dataset)
9. [Join V·ªõi Labels](#9-join-v·ªõi-labels)
10. [Export Dataset](#10-export-dataset)
11. [Monitoring & Maintenance](#11-monitoring--maintenance)
12. [Troubleshooting](#12-troubleshooting)

---

## 1. T·ªïng Quan

### 1.1. M·ª•c ƒê√≠ch

HDFS ƒë∆∞·ª£c d√πng ƒë·ªÉ:
- ‚úÖ L∆∞u tr·ªØ d·ªØ li·ªáu batch t·ª´ Spark Streaming (ƒë√£ gom theo 1 gi√¢y)
- ‚úÖ T·ªï ch·ª©c d·ªØ li·ªáu theo partition (year/month/day/hour/minute/second)
- ‚úÖ T·∫°o training dataset t·ª´ d·ªØ li·ªáu l·ªãch s·ª≠
- ‚úÖ Join v·ªõi labels t·ª´ `profile.txt`
- ‚úÖ Export dataset cho ML models (Parquet, CSV)

### 1.2. Ki·∫øn Tr√∫c

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Spark Streaming ‚îÇ  Ghi d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îÇ  (1 second windows)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Write Parquet files
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HDFS            ‚îÇ  Distributed storage
‚îÇ  (NameNode +     ‚îÇ  - Partitioned by time
‚îÇ   DataNodes)     ‚îÇ  - Parquet format
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Spark Batch Processing
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Training Dataset‚îÇ  - Join v·ªõi labels
‚îÇ  (Parquet/CSV)   ‚îÇ  - Feature engineering
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3. Lu·ªìng D·ªØ Li·ªáu

1. **Spark Streaming** ghi d·ªØ li·ªáu v√†o HDFS (m·ªói 1 gi√¢y)
2. **HDFS** l∆∞u tr·ªØ v·ªõi partitioning theo th·ªùi gian
3. **Spark Batch Job** ƒë·ªçc t·ª´ HDFS
4. **Join** v·ªõi labels t·ª´ `profile.txt`
5. **Feature Engineering** (aggregate 60 gi√¢y th√†nh 1 cycle)
6. **Export** training dataset (Parquet/CSV)

---

## 2. Y√™u C·∫ßu H·ªá Th·ªëng

### 2.1. Ph·∫ßn M·ªÅm

- **Java 8 ho·∫∑c 11** (required for Hadoop)
- **Hadoop 3.3.0+** (HDFS)
- **Python 3.8+** (cho PySpark)
- **Apache Spark** (ƒë√£ setup t·ª´ guide tr∆∞·ªõc)

### 2.2. T√†i Nguy√™n

- **RAM:** T·ªëi thi·ªÉu 4GB (khuy·∫øn ngh·ªã 8GB+)
- **CPU:** 2+ cores
- **Disk:** 50GB+ free space (cho data storage)

### 2.3. Dependencies

C·∫ßn th√™m v√†o `requirements.txt`:
```
pyspark>=3.3.0
pyarrow>=10.0.0  # Cho Parquet support
```

---

## 3. C√†i ƒê·∫∑t HDFS

### 3.1. Download Hadoop

```bash
# T·∫°o th∆∞ m·ª•c cho Hadoop
mkdir -p ~/hadoop
cd ~/hadoop

# Download Hadoop 3.3.6 (ho·∫∑c version m·ªõi nh·∫•t)
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Gi·∫£i n√©n
tar -xzf hadoop-3.3.6.tar.gz
cd hadoop-3.3.6

# Set environment variables
export HADOOP_HOME=$(pwd)
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

### 3.2. C·∫•u H√¨nh Java

```bash
# T√¨m Java home
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")

# Verify
echo $JAVA_HOME
java -version
```

### 3.3. C·∫•u H√¨nh SSH (cho distributed mode)

```bash
# Generate SSH key (n·∫øu ch∆∞a c√≥)
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# Copy key
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# Test SSH
ssh localhost
```

---

## 4. C·∫•u H√¨nh HDFS

### 4.1. C·∫•u H√¨nh Core

T·∫°o/s·ª≠a file `$HADOOP_HOME/etc/hadoop/core-site.xml`:

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/tmp/hadoop-${user.name}</value>
    </property>
</configuration>
```

### 4.2. C·∫•u H√¨nh HDFS

T·∫°o/s·ª≠a file `$HADOOP_HOME/etc/hadoop/hdfs-site.xml`:

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/tmp/hadoop-${user.name}/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/tmp/hadoop-${user.name}/datanode</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>
```

### 4.3. C·∫•u H√¨nh YARN (Optional, cho Spark)

T·∫°o/s·ª≠a file `$HADOOP_HOME/etc/hadoop/yarn-site.xml`:

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

### 4.4. Format HDFS

```bash
# Format namenode (ch·ªâ ch·∫°y l·∫ßn ƒë·∫ßu!)
$HADOOP_HOME/bin/hdfs namenode -format
```

‚ö†Ô∏è **Warning:** Ch·ªâ format l·∫ßn ƒë·∫ßu. Format l·∫°i s·∫Ω m·∫•t t·∫•t c·∫£ d·ªØ li·ªáu!

### 4.5. Start HDFS

```bash
# Start NameNode v√† DataNode
$HADOOP_HOME/sbin/start-dfs.sh

# Verify
jps
# Should see:
# - NameNode
# - DataNode
# - SecondaryNameNode (optional)
```

### 4.6. Verify HDFS

```bash
# List root directory
hdfs dfs -ls /

# T·∫°o test directory
hdfs dfs -mkdir -p /test
hdfs dfs -put README.md /test/
hdfs dfs -cat /test/README.md

# X√≥a test
hdfs dfs -rm -r /test
```

### 4.7. Web UI

Truy c·∫≠p HDFS Web UI:
```
http://localhost:9870
```

Xem:
- Cluster overview
- Browse file system
- NameNode information

---

## 5. C·∫•u Tr√∫c D·ªØ Li·ªáu

### 5.1. Directory Structure

```
hdfs://localhost:9000/
  /hydraulic_data/
    /streaming/              # D·ªØ li·ªáu t·ª´ Spark Streaming
      /year=2025/
        /month=11/
          /day=08/
            /hour=14/
              /minute=30/
                /second=45/
                  /cycle=0/
                    part-00000.parquet
                    part-00001.parquet
                    ...
    /batch/                  # D·ªØ li·ªáu batch processing
      /year=2025/
        /month=11/
          /day=08/
            /hour=14/
              /minute=30/
                /cycle=0/
                  aggregated.parquet
    /training/               # Training datasets
      /raw/                  # Raw features
      /processed/            # Processed features
      /labeled/              # With labels
```

### 5.2. Parquet File Schema

M·ªói Parquet file ch·ª©a:

```python
{
  "window_start": "2025-11-08T14:30:45",
  "window_end": "2025-11-08T14:30:46",
  "cycle": 0,
  "second_in_cycle": 45,
  
  # 100Hz sensors (100 samples/gi√¢y)
  "PS1_values": [151.19, 152.33, ..., 153.45],  # Array[100]
  "PS1_avg": 152.5,
  "PS1_min": 151.19,
  "PS1_max": 153.45,
  "PS1_std": 0.8,
  "PS1_count": 100,
  
  # ... (t∆∞∆°ng t·ª± cho PS2-6, EPS1)
  
  # 10Hz sensors (10 samples/gi√¢y)
  "FS1_values": [12.5, 12.6, ..., 12.9],  # Array[10]
  "FS1_avg": 12.7,
  "FS1_min": 12.5,
  "FS1_max": 12.9,
  "FS1_std": 0.1,
  "FS1_count": 10,
  
  # ... (t∆∞∆°ng t·ª± cho FS2)
  
  # 1Hz sensors (1 sample/gi√¢y)
  "TS1_avg": 45.2,  # Single value
  "TS1_min": 45.2,
  "TS1_max": 45.2,
  "TS1_std": 0.0,
  "TS1_count": 1,
  
  # ... (t∆∞∆°ng t·ª± cho TS2-4, VS1, CE, CP, SE)
  
  # Calculated features
  "pressure_mean": 152.5,
  "pressure_std": 0.8,
  "temperature_mean": 45.2,
  "flow_mean": 12.7,
  "pressure_rate_of_change": 0.5,
  ...
}
```

---

## 6. Ghi D·ªØ Li·ªáu V√†o HDFS

### 6.1. T·ª´ Spark Streaming

Xem ph·∫ßn 8 trong `SPARK_STREAMING_SETUP.md` ƒë·ªÉ ghi t·ª´ Spark Streaming.

### 6.2. T·ª´ Spark Batch Job

T·∫°o file `src/spark_batch_write.py`:

```python
#!/usr/bin/env python3
"""
Spark Batch Job - Ghi d·ªØ li·ªáu v√†o HDFS
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from datetime import datetime

def create_spark_session():
    """T·∫°o SparkSession"""
    spark = SparkSession.builder \
        .appName("HydraulicSystemBatchWrite") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def write_to_hdfs(df, output_path, partition_cols=None):
    """Ghi DataFrame v√†o HDFS d·∫°ng Parquet"""
    
    writer = df.write \
        .mode("overwrite") \
        .format("parquet") \
        .option("compression", "snappy")
    
    if partition_cols:
        writer = writer.partitionBy(*partition_cols)
    
    writer.save(output_path)
    
    print(f"‚úÖ Written to {output_path}")


def main():
    """Main function"""
    spark = create_spark_session()
    
    # V√≠ d·ª•: ƒê·ªçc t·ª´ source v√† ghi v√†o HDFS
    # (Thay b·∫±ng source th·ª±c t·∫ø c·ªßa b·∫°n)
    
    # T·∫°o sample data
    from pyspark.sql.types import *
    
    schema = StructType([
        StructField("window_start", TimestampType(), True),
        StructField("cycle", IntegerType(), True),
        StructField("PS1_avg", DoubleType(), True),
        StructField("PS2_avg", DoubleType(), True),
        # ... th√™m c√°c fields kh√°c
    ])
    
    # Sample data
    data = [
        (datetime(2025, 11, 8, 14, 30, 45), 0, 151.19, 178.41),
        (datetime(2025, 11, 8, 14, 30, 46), 0, 152.33, 179.22),
        # ...
    ]
    
    df = spark.createDataFrame(data, schema)
    
    # Th√™m partition columns
    df_partitioned = df \
        .withColumn("year", year("window_start")) \
        .withColumn("month", month("window_start")) \
        .withColumn("day", dayofmonth("window_start")) \
        .withColumn("hour", hour("window_start")) \
        .withColumn("minute", minute("window_start")) \
        .withColumn("second", second("window_start"))
    
    # Ghi v√†o HDFS
    output_path = "hdfs://localhost:9000/hydraulic_data/batch"
    write_to_hdfs(
        df_partitioned,
        output_path,
        partition_cols=["year", "month", "day", "hour", "minute", "second", "cycle"]
    )
    
    spark.stop()


if __name__ == "__main__":
    main()
```

### 6.3. Verify Data

```bash
# List files
hdfs dfs -ls -R /hydraulic_data/batch/

# Check file size
hdfs dfs -du -h /hydraulic_data/batch/

# Read sample (n·∫øu c√≥ parquet-tools)
parquet-tools head hdfs://localhost:9000/hydraulic_data/batch/year=2025/month=11/day=08/hour=14/minute=30/second=45/cycle=0/part-00000.parquet
```

---

## 7. ƒê·ªçc D·ªØ Li·ªáu T·ª´ HDFS

### 7.1. ƒê·ªçc v·ªõi Spark

T·∫°o file `src/spark_batch_read.py`:

```python
#!/usr/bin/env python3
"""
Spark Batch Job - ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def create_spark_session():
    """T·∫°o SparkSession"""
    spark = SparkSession.builder \
        .appName("HydraulicSystemBatchRead") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def read_from_hdfs(spark, hdfs_path, filters=None):
    """ƒê·ªçc Parquet files t·ª´ HDFS"""
    
    df = spark.read.parquet(hdfs_path)
    
    # Apply filters n·∫øu c√≥
    if filters:
        for col_name, value in filters.items():
            df = df.filter(col(col_name) == value)
    
    return df


def main():
    """Main function"""
    spark = create_spark_session()
    
    # ƒê·ªçc t·ª´ HDFS
    hdfs_path = "hdfs://localhost:9000/hydraulic_data/batch"
    
    # ƒê·ªçc t·∫•t c·∫£
    df = read_from_hdfs(spark, hdfs_path)
    
    print(f"‚úÖ Read {df.count()} records")
    print("\nSchema:")
    df.printSchema()
    
    print("\nSample data:")
    df.show(5, truncate=False)
    
    # ƒê·ªçc v·ªõi filter (partition pruning)
    df_filtered = read_from_hdfs(
        spark,
        hdfs_path,
        filters={"year": 2025, "month": 11, "day": 8, "cycle": 0}
    )
    
    print(f"\n‚úÖ Filtered: {df_filtered.count()} records")
    
    spark.stop()


if __name__ == "__main__":
    main()
```

### 7.2. Query v·ªõi Spark SQL

```python
# Register as table
df.createOrReplaceTempView("hydraulic_data")

# Query
result = spark.sql("""
    SELECT 
        cycle,
        AVG(PS1_avg) as avg_pressure,
        MAX(PS1_max) as max_pressure,
        COUNT(*) as record_count
    FROM hydraulic_data
    WHERE year = 2025 AND month = 11 AND day = 8
    GROUP BY cycle
    ORDER BY cycle
""")

result.show()
```

---

## 8. T·∫°o Training Dataset

### 8.1. Aggregate 60 Gi√¢y Th√†nh 1 Cycle

T·∫°o file `src/create_training_dataset.py`:

```python
#!/usr/bin/env python3
"""
T·∫°o Training Dataset t·ª´ HDFS
- Aggregate 60 gi√¢y th√†nh 1 cycle
- Join v·ªõi labels
- Feature engineering
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import pandas as pd

def create_spark_session():
    """T·∫°o SparkSession"""
    spark = SparkSession.builder \
        .appName("CreateTrainingDataset") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def load_labels(spark, labels_path):
    """Load labels t·ª´ profile.txt"""
    # ƒê·ªçc file local (ho·∫∑c t·ª´ HDFS)
    labels_df = pd.read_csv(labels_path, sep='\t', header=None)
    labels_df.columns = [
        'cycle',
        'cooler_condition',
        'valve_condition',
        'pump_leakage',
        'accumulator_pressure',
        'stable_flag'
    ]
    
    # Convert to Spark DataFrame
    labels_spark = spark.createDataFrame(labels_df)
    
    return labels_spark


def aggregate_by_cycle(df):
    """Aggregate 60 gi√¢y th√†nh 1 cycle"""
    
    # Group by cycle v√† t√≠nh to√°n features
    df_aggregated = df \
        .groupBy("cycle") \
        .agg(
            # Pressure sensors (100Hz)
            avg("PS1_avg").alias("PS1_mean"),
            stddev("PS1_avg").alias("PS1_std"),
            min("PS1_min").alias("PS1_min"),
            max("PS1_max").alias("PS1_max"),
            # ... t∆∞∆°ng t·ª± cho PS2-6, EPS1
            
            # Flow sensors (10Hz)
            avg("FS1_avg").alias("FS1_mean"),
            stddev("FS1_avg").alias("FS1_std"),
            # ... t∆∞∆°ng t·ª± cho FS2
            
            # Temperature sensors (1Hz)
            avg("TS1_avg").alias("TS1_mean"),
            stddev("TS1_avg").alias("TS1_std"),
            # ... t∆∞∆°ng t·ª± cho TS2-4, VS1, CE, CP, SE
            
            # Calculated features
            avg("pressure_mean").alias("pressure_mean"),
            stddev("pressure_mean").alias("pressure_std"),
            avg("temperature_mean").alias("temperature_mean"),
            avg("flow_mean").alias("flow_mean"),
            
            # Count
            count("*").alias("second_count")  # Should be 60
        )
    
    return df_aggregated


def calculate_cycle_features(df):
    """T√≠nh to√°n features cho m·ªói cycle"""
    
    df_features = df \
        .withColumn("pressure_range",
            col("PS1_max") - col("PS1_min")
        ) \
        .withColumn("temperature_range",
            col("TS1_max") - col("TS1_min")
        ) \
        .withColumn("pressure_variability",
            col("PS1_std") / (col("PS1_mean") + 1e-6)
        ) \
        .withColumn("flow_pressure_ratio",
            col("FS1_mean") / (col("pressure_mean") + 1e-6)
        )
    
    return df_features


def join_with_labels(df_features, labels_df):
    """Join v·ªõi labels"""
    
    df_labeled = df_features \
        .join(labels_df, "cycle", "inner") \
        .orderBy("cycle")
    
    return df_labeled


def main():
    """Main function"""
    spark = create_spark_session()
    
    # 1. ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
    print("üìä Reading data from HDFS...")
    hdfs_path = "hdfs://localhost:9000/hydraulic_data/batch"
    df = spark.read.parquet(hdfs_path)
    
    print(f"‚úÖ Read {df.count()} records")
    
    # 2. Aggregate theo cycle
    print("\nüîÑ Aggregating by cycle...")
    df_aggregated = aggregate_by_cycle(df)
    
    print(f"‚úÖ Aggregated to {df_aggregated.count()} cycles")
    
    # 3. T√≠nh to√°n features
    print("\nüßÆ Calculating features...")
    df_features = calculate_cycle_features(df_aggregated)
    
    # 4. Load labels
    print("\nüìã Loading labels...")
    labels_path = "data/profile.txt"
    labels_df = load_labels(spark, labels_path)
    
    print(f"‚úÖ Loaded {labels_df.count()} labels")
    
    # 5. Join v·ªõi labels
    print("\nüîó Joining with labels...")
    df_labeled = join_with_labels(df_features, labels_df)
    
    print(f"‚úÖ Joined dataset: {df_labeled.count()} cycles")
    
    # 6. Verify
    print("\nüìä Sample data:")
    df_labeled.show(5, truncate=False)
    
    print("\nüìà Statistics:")
    df_labeled.describe().show()
    
    # 7. Ghi training dataset
    print("\nüíæ Writing training dataset...")
    output_path = "hdfs://localhost:9000/hydraulic_data/training/labeled"
    
    df_labeled.write \
        .mode("overwrite") \
        .format("parquet") \
        .option("compression", "snappy") \
        .save(output_path)
    
    print(f"‚úÖ Written to {output_path}")
    
    # 8. Export CSV (optional)
    print("\nüìÑ Exporting CSV...")
    csv_path = "hdfs://localhost:9000/hydraulic_data/training/csv"
    
    df_labeled.write \
        .mode("overwrite") \
        .format("csv") \
        .option("header", "true") \
        .save(csv_path)
    
    print(f"‚úÖ Written CSV to {csv_path}")
    
    spark.stop()


if __name__ == "__main__":
    main()
```

---

## 9. Join V·ªõi Labels

### 9.1. Load Labels t·ª´ File

```python
def load_labels_from_file(spark, file_path):
    """Load labels t·ª´ profile.txt"""
    
    # ƒê·ªçc file local
    labels_pd = pd.read_csv(
        file_path,
        sep='\t',
        header=None,
        names=[
            'cycle',
            'cooler_condition',
            'valve_condition',
            'pump_leakage',
            'accumulator_pressure',
            'stable_flag'
        ]
    )
    
    # Convert to Spark DataFrame
    labels_df = spark.createDataFrame(labels_pd)
    
    return labels_df
```

### 9.2. Load Labels t·ª´ HDFS

```python
def load_labels_from_hdfs(spark, hdfs_path):
    """Load labels t·ª´ HDFS"""
    
    labels_df = spark.read \
        .option("sep", "\t") \
        .option("header", "false") \
        .csv(hdfs_path)
    
    # Rename columns
    labels_df = labels_df.select(
        col("_c0").alias("cycle").cast(IntegerType()),
        col("_c1").alias("cooler_condition").cast(IntegerType()),
        col("_c2").alias("valve_condition").cast(IntegerType()),
        col("_c3").alias("pump_leakage").cast(IntegerType()),
        col("_c4").alias("accumulator_pressure").cast(IntegerType()),
        col("_c5").alias("stable_flag").cast(IntegerType())
    )
    
    return labels_df
```

### 9.3. Join

```python
# Inner join (ch·ªâ cycles c√≥ labels)
df_labeled = df_features.join(labels_df, "cycle", "inner")

# Left join (gi·ªØ t·∫•t c·∫£ cycles, null cho cycles kh√¥ng c√≥ labels)
df_labeled = df_features.join(labels_df, "cycle", "left")

# Verify join
print(f"Features: {df_features.count()}")
print(f"Labels: {labels_df.count()}")
print(f"Joined: {df_labeled.count()}")
```

---

## 10. Export Dataset

### 10.1. Export Parquet

```python
df_labeled.write \
    .mode("overwrite") \
    .format("parquet") \
    .option("compression", "snappy") \
    .save("hdfs://localhost:9000/hydraulic_data/training/labeled")
```

### 10.2. Export CSV

```python
df_labeled.write \
    .mode("overwrite") \
    .format("csv") \
    .option("header", "true") \
    .option("sep", ",") \
    .save("hdfs://localhost:9000/hydraulic_data/training/csv")
```

### 10.3. Export to Local

```python
# Copy t·ª´ HDFS v·ªÅ local
df_labeled.coalesce(1).write \
    .mode("overwrite") \
    .format("csv") \
    .option("header", "true") \
    .save("file:///path/to/local/training_dataset.csv")
```

### 10.4. Download t·ª´ HDFS

```bash
# Download Parquet
hdfs dfs -get /hydraulic_data/training/labeled /local/path/

# Download CSV
hdfs dfs -get /hydraulic_data/training/csv /local/path/
```

---

## 11. Monitoring & Maintenance

### 11.1. HDFS Health Check

```bash
# Check HDFS status
hdfs dfsadmin -report

# Check disk usage
hdfs dfs -du -h /hydraulic_data/

# Check file count
hdfs dfs -count /hydraulic_data/
```

### 11.2. Cleanup Old Data

```bash
# X√≥a d·ªØ li·ªáu c≈© (v√≠ d·ª•: > 30 ng√†y)
hdfs dfs -rm -r /hydraulic_data/streaming/year=2025/month=10/

# Ho·∫∑c d√πng retention policy
```

### 11.3. Backup

```bash
# Backup to another location
hdfs dfs -cp /hydraulic_data/training /backup/hydraulic_data/training
```

---

## 12. Troubleshooting

### 12.1. HDFS Not Starting

**L·ªói:** `NameNode not starting`

**Gi·∫£i ph√°p:**
```bash
# Check logs
tail -f $HADOOP_HOME/logs/hadoop-*-namenode-*.log

# Check Java
echo $JAVA_HOME

# Reformat (‚ö†Ô∏è m·∫•t d·ªØ li·ªáu!)
$HADOOP_HOME/bin/hdfs namenode -format
```

### 12.2. Permission Denied

**L·ªói:** `Permission denied`

**Gi·∫£i ph√°p:**
```bash
# Disable permissions (development only)
# ƒê√£ set trong hdfs-site.xml: dfs.permissions = false

# Ho·∫∑c set permissions
hdfs dfs -chmod -R 777 /hydraulic_data
```

### 12.3. Out of Space

**L·ªói:** `No space left on device`

**Gi·∫£i ph√°p:**
```bash
# Check disk usage
df -h

# Cleanup old data
hdfs dfs -rm -r /hydraulic_data/streaming/year=2024/

# Increase disk space
```

### 12.4. Slow Reads

**L·ªói:** `Slow query performance`

**Gi·∫£i ph√°p:**
- Use partition pruning (filter by partition columns)
- Use column pruning (select only needed columns)
- Use compression (snappy)
- Optimize file size (coalesce)

---

## 13. Best Practices

1. **Partitioning:** Partition theo th·ªùi gian ƒë·ªÉ query nhanh
2. **Compression:** D√πng Snappy cho Parquet
3. **File Size:** Gi·ªØ file size 128MB-256MB
4. **Schema Evolution:** D√πng schema registry n·∫øu c√≥
5. **Monitoring:** Monitor disk usage v√† health
6. **Backup:** Backup training datasets ƒë·ªãnh k·ª≥

---

## 14. Next Steps

Sau khi setup HDFS:
1. ‚úÖ Verify data flow t·ª´ Spark Streaming
2. ‚úÖ Test ƒë·ªçc/ghi operations
3. ‚úÖ T·∫°o training dataset
4. ‚úÖ Join v·ªõi labels
5. ‚úÖ Export cho ML models
6. ‚úÖ Setup monitoring

---

## üìö References

- [HDFS User Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
- [Parquet Format](https://parquet.apache.org/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

