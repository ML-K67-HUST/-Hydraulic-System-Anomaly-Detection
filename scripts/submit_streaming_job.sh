#!/bin/bash
# Submit Spark Streaming Job (streaming_job.py) to Spark cluster
# Theo káº¿ hoáº¡ch triá»ƒn khai chi tiáº¿t

echo "=========================================="
echo "ðŸš€ Submitting Spark Streaming Job"
echo "   (streaming_job.py - Dual Sink: HDFS + MongoDB)"
echo "=========================================="

cd "$(dirname "$0")/.."

# Configuration
SPARK_MASTER="spark://spark-master:7077"
APP_NAME="HydraulicSystemStreaming"
PYTHON_APP="src/streaming_job.py"
CHECKPOINT_DIR="/tmp/spark-checkpoints/streaming_job"

# Check if Spark services are running
echo ""
echo "ðŸ” Checking Spark services..."
if ! docker ps | grep -q "spark-master"; then
    echo "âŒ Spark master not running!"
    echo "   Start with: docker-compose -f docker-compose.khang.yml up -d spark-master spark-worker"
    exit 1
fi

if ! docker ps | grep -q "spark-worker"; then
    echo "âŒ Spark worker not running!"
    echo "   Start with: docker-compose -f docker-compose.khang.yml up -d spark-master spark-worker"
    exit 1
fi

echo "âœ… Spark services running"

# Check if Kafka is running
echo ""
echo "ðŸ” Checking Kafka..."
if ! docker ps | grep -q "kafka"; then
    echo "âŒ Kafka not running!"
    echo "   Start with: docker-compose -f docker-compose.khang.yml up -d kafka zookeeper"
    exit 1
fi

echo "âœ… Kafka running"

# Check if MongoDB is running (optional but recommended)
echo ""
echo "ðŸ” Checking MongoDB..."
if ! docker ps | grep -q "mongodb"; then
    echo "âš ï¸  MongoDB not running (optional for hot sink)"
    echo "   Start with: docker-compose -f docker-compose.khang.yml up -d mongodb"
else
    echo "âœ… MongoDB running"
fi

# Check if Python app exists
if [ ! -f "$PYTHON_APP" ]; then
    echo "âŒ Python app not found: $PYTHON_APP"
    exit 1
fi

# Check if config exists
if [ ! -f "config/spark/streaming.yaml" ]; then
    echo "âŒ Config file not found: config/spark/streaming.yaml"
    exit 1
fi

echo ""
echo "ðŸ“¤ Submitting job to Spark cluster..."
echo "   Master: $SPARK_MASTER"
echo "   App: $PYTHON_APP"
echo "   Config: config/spark/streaming.yaml"
echo ""

# Copy files to spark-apps directory for Docker
mkdir -p spark-apps
cp "$PYTHON_APP" spark-apps/
cp -r config spark-apps/ 2>/dev/null || true

# Submit job using spark-submit in Docker
# Packages needed:
# - spark-sql-kafka-0-10: Kafka integration
# - mongo-spark-connector: MongoDB integration
docker exec -it hydraulic-system-anomaly-detection-spark-master-1 \
    /opt/spark/bin/spark-submit \
    --master $SPARK_MASTER \
    --name $APP_NAME \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.0.0 \
    --conf spark.sql.streaming.checkpointLocation=$CHECKPOINT_DIR \
    --conf spark.sql.adaptive.enabled=true \
    --conf spark.driver.memory=2g \
    --conf spark.executor.memory=2g \
    --py-files /opt/spark-apps/config \
    /opt/spark-apps/streaming_job.py

echo ""
echo "=========================================="
echo "âœ… Job submitted!"
echo "=========================================="
echo ""
echo "ðŸ“Š Monitor at: http://localhost:8080"
echo "ðŸ›‘ To stop: Press Ctrl+C or kill the job"
echo ""

